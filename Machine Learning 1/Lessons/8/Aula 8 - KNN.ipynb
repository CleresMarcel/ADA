{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBjsuuG1pr3g"
   },
   "source": [
    "# Aula 8 - KNN\n",
    "\n",
    "Nessa aula, iremos tratar dos seguintes conteúdos:\n",
    "- KNN - Classificador;\n",
    "- KNN - Regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYrzOoJRpr3k"
   },
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Mohammed-Badawy/publication/331424423/figure/fig1/AS:732056359297024@1551547245072/Example-on-KNN-classifier.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yce7GlOpr3l"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfUqCYDFpr3l"
   },
   "source": [
    "## K Nearest Neighbors ou K-Vizinhos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yADvR3nlpr3m"
   },
   "source": [
    "O modelo K Nearest Neighbors ou K-Vizinhos (KNN) é um modelo de classificação bem simples, onde a ideia de aplicação do modelo é: iremos definir se um dado pertence ou não a uma classe, ou no caso multi classes para quais classes irá pertencer, a partir de cálculos de **distâncias!** <br><br>\n",
    "Os modelos de KNNs apresentam:\n",
    "- Algoritmo simples\n",
    "- Aprendizado supervisionado\n",
    "- Amplamente estudado\n",
    "    - Primeira (?) descrição por [Evelyn Fix e Joseph Hodges (1951)](https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf)\n",
    "    - Posteriormente expandido por [Thomas Cover (1967)](http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf) (Nearest Neighbor Pattern Classification)\n",
    "- Amplamente utilizado\n",
    "- Bom algoritmo para benchmark de performance\n",
    "- Navalha de Ockham (Occam's razor)\n",
    "    - **Diante de várias explicações para um problema, a mais simples tende a ser a mais correta**\n",
    "\n",
    "\n",
    "**Conceitos**:\n",
    "- Lazy learning\n",
    "    - Processamento dos exemplos de treino postergado até o modelo realizar as predições\n",
    "    - Menor tempo \"treinando\", maior tempo \"predizendo\"\n",
    "    - O treinamento consiste em armazenar o dado de treino (fit)!!!\n",
    "- Categorizado como instance-based (ou \"memory-based\")\n",
    "    - Comparação do ponto a ser classificado com o conjunto de dados presentes no set de treino.\n",
    "    - Diferente de um modelo global.\n",
    "\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1qtI7EuVeoCyoLLYBAqvLzojZpGyNFI5s\">\n",
    "\n",
    "\n",
    "\n",
    "A cada observação nova que devemos classificar, será feito o cálculo de distância das $K$ observações mais próximas em relação ao elemento a ser classificado e de acordo com a classificação desses vizinhos próximos, será definida a classe da observação: \n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\" width=600>\n",
    "\n",
    "É importante ressaltar que o modelo KNN é **bem simples** de rodar, tem um **custo computacional mais baixo** em relação a outros modelos, mas esse modelo **ele não aprende** com os dados já observados, toda vez que tivermos novos dados serão calculadas todas as ditâncias para definir os elementos mais próximos e assim conseguir classificá-lo.<br><br>\n",
    "\n",
    "Existem vários tipos de distâncias diferentes que podemos utilizar no KNN, sendo elas: [Euclidiana](https://en.wikipedia.org/wiki/Euclidean_distance), [Minkowski](https://en.wikipedia.org/wiki/Minkowski_distance), [Cosseno](https://cmry.github.io/notes/euclidean-v-cosine) e [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).<br><br>\n",
    "\n",
    "Então basicamente o nosso processo de modelagem envolve as seguintes etapas:\n",
    "\n",
    "1. Recebe um dado não classificado;\n",
    "2. Mede sua distância (Euclidiana, Manhattan, Minkowski ou Ponderada) de cada um dos elementos da base de treino;\n",
    "3. Obtém um *ranking* das distâncias, em ordem decrescente;\n",
    "4. Verifica a classe de cada da um dos *K* dados que tiveram a menor distância e conta a quantidade de cada classe;\n",
    "5. Toma como resultado a classe que mais apareceu dentre os dados que tiveram as menores distâncias;\n",
    "6. Classifica o novo dado com a classe tomada como resultado da classificação.\n",
    "\n",
    "Mas sempre que olhamos para o nosso processo de modelagem (como o exemplo acima), a primeira dúvida que vem na cabeça é:\n",
    "\n",
    "> Qual é o número de vizinhos adequados: i.e., **como escolher $k$?**\n",
    "\n",
    "Essa é uma escolha particularmente importante, pois escolhas diferentes de $k$ podem facilmente levar a classificações diferentes:\n",
    "\n",
    "<img src=\"https://helloacm.com/wp-content/uploads/2016/03/2012-10-26-knn-concept.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU_MbiVhbhGk"
   },
   "source": [
    "**Treinamento:**  \n",
    "Para cada i = 1, ..., n em um conjunto de dados de treino multidimensional:  \n",
    "- Armazene o exemplo de treino $(X^{[i]}, y^{[i]})$\n",
    "\n",
    "**Predição**\n",
    "Pseudo-código\n",
    "\n",
    "Para cada ponto do dado de treino,  i = 1, ..., n;\n",
    "- Descubra a distância $\\mathcal{d}(X^{[i]}, X^{[q]} )$, em que $\\mathcal{q}$ representa o ponto de teste;\n",
    "- Amrazene as distâncias em ordem da menor para maior;\n",
    "- Escolha os primeiros k pontos;\n",
    "- Conte a frequência de cada classe;\n",
    "    - Se não houve empate o ponto de teste pertence a classe de maior frequência;\n",
    "    - Se houver empate, selecione aleatoriamente uma classe;\n",
    "Fim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcucB45vbhGk"
   },
   "source": [
    "Exemplo de distâncias:\n",
    "- Distância euclidiana\n",
    "\n",
    "$\\mathcal{d}(X^{[a]}, X^{[b]}) = \\sqrt{ \\sum_{j=1}^n (x_{j}^{[a]} - x_{j}^{[b]} )^2   }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgeEsqdAbhGk"
   },
   "source": [
    "### 3.2 Maldição da dimensionalidade\n",
    "\n",
    "KNNs são particulamente sucetiveis a mandição da dimensionalidade.  \n",
    "Em ML, número fixo de exemplos de treino mas o aumento do número de dimensões (features). Aumento da dimensão maior o número de hiperespaços necessário para capturar um número fixo de vizinhos. Com o aumento do volume, os vizinhos começam a ficar vez menos similar ao ponto de teste (query), dessa forma prejudicando a performance do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6KblfL5pr3q"
   },
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u26Jteetpr3r"
   },
   "source": [
    "## Exemplo Prático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6hq2NuYpr33"
   },
   "source": [
    "## KNN como Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkXvXpCZpr39"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNUsfSVupr3-"
   },
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eyGxMQNpr3-"
   },
   "source": [
    "Utilizando a base de dados do `insurance.csv`, resolva os seguintes exercícios:\n",
    "\n",
    "- __1)__ Utilize um KNN classificador para definir as classes de _charges_ de acordo com as variáveis. Será preciso construir a target para esse exercício, então utilize o _jointplot_ entre a idade e o _charges_ para definir 3 regiões de separação;\n",
    "\n",
    "- __2)__ Agora utilize um KNN regressor para definir os valores de _charges_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CunpsFEspr3-",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHd9Njf5pr3-",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIaBPZ9Upr3-",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "joSwE73xpr3s",
    "OI3aPtQspr3z",
    "b7TO3YXVpr32"
   ],
   "name": "Aula 4 - KNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
