{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrkNLVl5x2IZ"
   },
   "source": [
    "# Aula 3 - Ensembles\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Métodos de Ensembles - Bagging e Boosting\n",
    "- 2) Random Forest\n",
    "- 3) Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTAySOC-x2Id"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/899/0*Wy3QjtXL9qf-Ssyz.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn7Se-lGx2Ie"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ohCjusx2Ie"
   },
   "source": [
    "## Métodos de Ensemble\n",
    "\n",
    "\n",
    "Há uma classe de algoritmos de Machine Learning, os chamados **métodos de ensemble** que tem como objetivo **combinar as predições de diversos estimadores mais simples** para gerar uma **predição final mais robusta**\n",
    "\n",
    "Os métodos de ensemble são ainda divididos em duas classes:\n",
    "\n",
    "- **Métodos de Bagging**: têm como procedimento geral construir diversos estimadores independentes, e tomar a média de suas predições como a predição final. O principal objetivo do método é reduzir variância, de modo que o modelo final seja melhor que todos os modelos individuais. Ex.: **random forest.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Métodos de Boosting**: têm como procedimento geral a construção de estimadores de forma sequencial, de modo que estimadores posteriores tentam reduzir o viés do estimador conjunto, que leva em consideração estimadores anteriores. Ex.: **adaboost**.\n",
    "\n",
    "Para mais detalhes, [clique aqui!](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "**Outros tipos de ensembles**, pode ser a combinação de diferentes modelos (note que diferentes modelos pode ser dois modelos de Random Forest com parâmetros diferentes). Para mais detalhes [clique aqui](https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtuL7rD_JUyf"
   },
   "source": [
    "### Por quê utilizar Ensembles no lugar de Árvores de decisão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RAzx190JUyf"
   },
   "source": [
    "Citando a biblia de machine learning, **The elements of Statistical Learning**. \"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely **inaccuracy**\".  \n",
    "Em outras palavras, elas são ótimas para os dados que criaram o modelo, mas ***não são flexiveis para classificar novas amostras*** (trade-off bias vs variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdr5EQ-tx2If"
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "Uma técnica muito interessante baseada em árvores é o **Random Forest**. Neste modelo, são criadas varias árvores usando uma seleção aleatória de features, e calculado para uma das árvores. **Combinando**, dessa forma, **a simplicidade simplicidade das árvores de decisão com flexibilidade resultando num grande aumento da acurácia**.\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/goPiwckWE9M/maxresdefault.jpg\" width=700>\n",
    "\n",
    "O modelo de RandomForest utiliza os conceitos de **bootstraping** e **aggregation** (ou então, o procedimento composto **bagging**) para criar um modelo composto que é melhor que uma única árvore!. Vamos explicar cada uma das etapas:<br><br>\n",
    "- **Bootstraping**:\n",
    "    - Usando os dados originais, cria-se uma amostra desses dados, que pode haver ou não **repetições**;<br>\n",
    "    - Utilizando o bootstrapped dataset, consideramos apenas um subset de features para cada passo (nó). \n",
    "   \n",
    "- **Aggregation**:\n",
    "    - Agregamos os resultados obtidos de cada árvore de decisão selecionadas no passo de Boostraping.\n",
    "        - Para obter a predição no final, no caso de classificadores observa-se qual a classe mais frequente na análise.\n",
    "        - No caso de regressores retorna a média das respostas das diversas árvores (**SIM** *RandomForest* pode ser aplicada em Regressões).\n",
    "        \n",
    "A junção de **B**ostrapping dos dados com o uso de **agg**regate para tomada de decisão é chamada de ***Bagg***ing\n",
    "\n",
    "<img src=\"https://c.mql5.com/2/33/image1__1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKWW7nizJUyh"
   },
   "source": [
    "**Como sabemos se as árvores formadas são boas?**  \n",
    "Quando criamos o bootstrapped dataset, permitimos a duplicação de dados (repetição). Em geral, deixamos 1/3 dos dados fora dos dados fora do boostrapped dataset. Esses dados são chamados de ***Out-of-Bag-Dataset*** (que são os dados que não aparecem no bootstrapped dataset).\n",
    "\n",
    "Como esses dados não foram utilizados para montar a árvore de decisão, podemos utilizá-los para verificar se as árvores de decisão classificam corretamente as amostras.\n",
    "\n",
    "Portanto, podemos medir a acurácia da RF pela proporção de amostras **Out-of-Bag** que foram corretamente classificadas pela RF.\n",
    "\n",
    "Note que, a proporção de amostras **Out-of-Bag** que foram classificadas de forma errada são chamadas de **Out-of-Bag Error**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxBH7A5ox2If"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcLRwGsvx2If"
   },
   "source": [
    "### Exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCbStrq3x2Ig"
   },
   "source": [
    "Partindo do mesmo exemplo da aula anterior para _Heart Failure_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkwNv3DGx2Im"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEjqPjDJUyl"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGozV9hlx2In"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-VCi_bIx2In"
   },
   "source": [
    "### Adaboost\n",
    "\n",
    "O Adaboost significa **Adaptive Boosting**, e tem como procedimento geral **a criação sucessiva de árvores de um único nó (stumps - modelos fracos) que utiliza dos erros da árvore anterior para melhorar a próxima árvore**. As predições finais são feitas com base **nos pesos de cada stump**, cuja determinação faz parte do algoritmo.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1744/1*nJ5VrsiS1yaOR77d4h8gyw.png\" width=300>\n",
    "\n",
    "\n",
    "Enquanto na RF cada árvore tem um peso igual (todas votam da mesma foram) para a decisão final, no AdaBoost, alguns *stumps* apresentam um peso maior para a classificação do que outros.\n",
    "\n",
    "Outra diferença é que nas RFs as árvores são criadas de forma independente, enquanto no AdaBoost o erro que o primeiro stump comete influência como o segundo stump é formado, e assim sucessivamente. \n",
    "\n",
    "\n",
    "1-) AdaBoost combinada diversos **modelos fracos** (*weak learnes*).  \n",
    "2-) Alguns stumps recebem um maior peso para classificação do que outros  \n",
    "3-) Cada stump é formado considerando os erros do stump anterior  \n",
    "\n",
    "O primeiro stump é formado utilizando indice de Gini, selecionando qual feature apresenta a maior pureza (com uma quebra apenas). Nós determinamos quanto o stump tem de peso na classificação final utilizando a formula\n",
    "\n",
    "$AmountOfSay = \\frac{1}{2} log (\\frac{1 - Total Error}{Total Error})$\n",
    "\n",
    "O interessante é que no inicio, todas as amostras apresentam o mesmo peso para o **Erro Total**. Porém nas próximas iterações, os stumps consideram os erros cometidos no stump anterior. Significando que iremos aumentar o peso das amostras que foram classificadas de forma errada utilizando a formula a seguir:\n",
    "\n",
    "$ NewSampleWeight = SampleWeight * e^{AmountOfSay}$\n",
    "\n",
    "Para as amostras que foram classificadas de forma correta utilizamos a seguinte formula para recalcular o peso\n",
    "\n",
    "$NewSampleWeight = SampleWeight * e^{-AmountOfSay}$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhuo_Wang8/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png\" width=500>\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOr29K_Lx2Io"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyHwvfIDx2It"
   },
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5KJuBdgx2It"
   },
   "source": [
    "__1)__ Realize uma classificação com da coluna y dos dados de marketing bancário (contido no arquivo `bank-full.csv`) utilizando mais de um modelo dentro dos modelos conhecidos por vocês. Qual o melhor modelo a ser implementado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsUsX9uVx2It"
   },
   "source": [
    "__2)__ Com os dados do arquivo `Admission_Predict.csv` realize uma classificação dos alunos que possuem chance maior que 0.8 de admissão e dos que possuem chance menor que 0.8. Compare diversos modelos e determine qual seria o melhor a ser implementado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI8QYsAWx2It"
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WkwNv3DGx2Im",
    "yGozV9hlx2In",
    "rC3_mqoZx2Is"
   ],
   "name": "Aula 3 Ensembles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
