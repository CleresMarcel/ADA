{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrkNLVl5x2IZ"
   },
   "source": [
    "# Aula 3 - Ensembles\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Métodos de Ensembles - Bagging e Boosting\n",
    "- 2) Random Forest\n",
    "- 3) Adaboost\n",
    "- 4) XGBoost\n",
    "- 5) LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTAySOC-x2Id"
   },
   "source": [
    "<img src=\"https://images.datacamp.com/image/upload/v1700592030/image7_3e2d0cd6d9.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn7Se-lGx2Ie"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58ohCjusx2Ie"
   },
   "source": [
    "## Métodos de Ensemble\n",
    "\n",
    "\n",
    "Há uma classe de algoritmos de Machine Learning, os chamados **métodos de ensemble** que tem como objetivo **combinar as predições de diversos estimadores mais simples** para gerar uma **predição final mais robusta**\n",
    "\n",
    "Os métodos de ensemble são ainda divididos em duas classes:\n",
    "\n",
    "- **Métodos de Bagging**: têm como procedimento geral construir diversos estimadores independentes, e tomar a média de suas predições como a predição final. O principal objetivo do método é reduzir variância, de modo que o modelo final seja melhor que todos os modelos individuais. Ex.: **random forest.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Métodos de Boosting**: têm como procedimento geral a construção de estimadores de forma sequencial, de modo que estimadores posteriores tentam reduzir o viés do estimador conjunto, que leva em consideração estimadores anteriores. Ex.: **adaboost**.\n",
    "\n",
    "Para mais detalhes, [clique aqui!](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "**Outros tipos de ensembles**, pode ser a combinação de diferentes modelos (note que diferentes modelos pode ser dois modelos de Random Forest com parâmetros diferentes). Para mais detalhes [clique aqui](https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "![image](https://images.datacamp.com/image/upload/v1700592080/image3_78e8da325b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado combinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtuL7rD_JUyf"
   },
   "source": [
    "![](https://s3-sa-east-1.amazonaws.com/lcpi/fece1b59-5854-417e-adbf-f9aca88d75ed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdr5EQ-tx2If"
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "Uma técnica muito interessante baseada em árvores é o **Random Forest**. Neste modelo, são criadas varias árvores usando uma seleção aleatória de features, e calculado para uma das árvores. **Combinando**, dessa forma, **a simplicidade simplicidade das árvores de decisão com flexibilidade resultando num grande aumento da acurácia**.\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/goPiwckWE9M/maxresdefault.jpg\" width=700>\n",
    "\n",
    "#### BAGGING: Estratégia de Divisão e Conquista\n",
    "O modelo de RandomForest utiliza os conceitos de **bootstraping** e **aggregation** (ou então, o procedimento composto **bagging**) para criar um modelo composto que é melhor que uma única árvore!. Vamos explicar cada uma das etapas:<br><br>\n",
    "- **Bootstraping**:\n",
    "    - Usando os dados originais, cria-se uma amostra desses dados, que pode haver ou não **repetições**;<br>\n",
    "    - Utilizando o bootstrapped dataset, consideramos apenas um subset de features para cada passo (nó). \n",
    "   \n",
    "- **Aggregation**:\n",
    "    - Agregamos os resultados obtidos de cada árvore de decisão selecionadas no passo de Boostraping.\n",
    "        - Para obter a predição no final, no caso de classificadores observa-se qual a classe mais frequente na análise.\n",
    "        - No caso de regressores retorna a média das respostas das diversas árvores (**SIM** *RandomForest* pode ser aplicada em Regressões).\n",
    "        \n",
    "A junção de **B**ostrapping dos dados com o uso de **agg**regate para tomada de decisão é chamada de ***Bagg***ing\n",
    "\n",
    "<img src=\"https://c.mql5.com/2/33/image1__1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKWW7nizJUyh"
   },
   "source": [
    "#### Por quê utilizar Ensembles no lugar de Árvores de decisão?\n",
    "\n",
    "Citando a biblia de machine learning, **The elements of Statistical Learning**. \"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely **inaccuracy**\".  \n",
    "Em outras palavras, elas são ótimas para os dados que criaram o modelo, mas ***não são flexiveis para classificar novas amostras*** (trade-off bias vs variance).\n",
    "\n",
    "**Como sabemos se as árvores formadas são boas?**  \n",
    "Quando criamos o bootstrapped dataset, permitimos a duplicação de dados (repetição). Em geral, deixamos 1/3 dos dados fora dos dados fora do boostrapped dataset. Esses dados são chamados de ***Out-of-Bag-Dataset*** (que são os dados que não aparecem no bootstrapped dataset).\n",
    "\n",
    "Como esses dados não foram utilizados para montar a árvore de decisão, podemos utilizá-los para verificar se as árvores de decisão classificam corretamente as amostras.\n",
    "\n",
    "Portanto, podemos medir a acurácia da RF pela proporção de amostras **Out-of-Bag** que foram corretamente classificadas pela RF.\n",
    "\n",
    "Note que, a proporção de amostras **Out-of-Bag** que foram classificadas de forma errada são chamadas de **Out-of-Bag Error**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxBH7A5ox2If"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcLRwGsvx2If"
   },
   "source": [
    "### Exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCbStrq3x2Ig"
   },
   "source": [
    "Partindo do mesmo exemplo da aula anterior para _Heart Failure_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T21:44:37.669368Z",
     "start_time": "2020-02-16T21:44:32.974848Z"
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1644012040822,
     "user": {
      "displayName": "Gilberto Kaihami",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg7889GQh1x2Zx39afauWPJZMRy7nevpgXX_pwq7Q=s64",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "D06eIt23x2Ih"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1644012040823,
     "user": {
      "displayName": "Gilberto Kaihami",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg7889GQh1x2Zx39afauWPJZMRy7nevpgXX_pwq7Q=s64",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "uAVMesHZx2Ii"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_failure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGozV9hlx2In"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-VCi_bIx2In"
   },
   "source": [
    "### Boosting - Da prática à perfeição\n",
    "\n",
    "![](https://images.datacamp.com/image/upload/v1700592126/image1_fcace6f2b3.png)\n",
    "\n",
    "#### Adaboost\n",
    "\n",
    "O Adaboost significa **Adaptive Boosting**, e tem como procedimento geral **a criação sucessiva de árvores de um único nó (stumps - modelos fracos) que utiliza dos erros da árvore anterior para melhorar a próxima árvore**. As predições finais são feitas com base **nos pesos de cada stump**, cuja determinação faz parte do algoritmo.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1744/1*nJ5VrsiS1yaOR77d4h8gyw.png\" width=300>\n",
    "\n",
    "\n",
    "Enquanto na RF cada árvore tem um peso igual (todas votam da mesma foram) para a decisão final, no AdaBoost, alguns *stumps* apresentam um peso maior para a classificação do que outros.\n",
    "\n",
    "Outra diferença é que nas RFs as árvores são criadas de forma independente, enquanto no AdaBoost o erro que o primeiro stump comete influência como o segundo stump é formado, e assim sucessivamente. \n",
    "\n",
    "\n",
    "1-) AdaBoost combinada diversos **modelos fracos** (*weak learnes*).  \n",
    "2-) Alguns stumps recebem um maior peso para classificação do que outros  \n",
    "3-) Cada stump é formado considerando os erros do stump anterior  \n",
    "\n",
    "O primeiro stump é formado utilizando indice de Gini, selecionando qual feature apresenta a maior pureza (com uma quebra apenas). Nós determinamos quanto o stump tem de peso na classificação final utilizando a formula\n",
    "\n",
    "$AmountOfSay = \\frac{1}{2} log (\\frac{1 - Total Error}{Total Error})$\n",
    "\n",
    "O interessante é que no inicio, todas as amostras apresentam o mesmo peso para o **Erro Total**. Porém nas próximas iterações, os stumps consideram os erros cometidos no stump anterior. Significando que iremos aumentar o peso das amostras que foram classificadas de forma errada utilizando a formula a seguir:\n",
    "\n",
    "$ NewSampleWeight = SampleWeight * e^{AmountOfSay}$\n",
    "\n",
    "Para as amostras que foram classificadas de forma correta utilizamos a seguinte formula para recalcular o peso\n",
    "\n",
    "$NewSampleWeight = SampleWeight * e^{-AmountOfSay}$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhuo_Wang8/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png\" width=500>\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOr29K_Lx2Io"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azyix6ihx2Io"
   },
   "source": [
    "## GradientBoosting e XGBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTkw9z1Ax2Io"
   },
   "source": [
    "Similar ao AdaBoost o GradientBoost utiliza árvores mais simples que uma RF, porém são maiores que um stump, limitando o número de folhas (em geral de 8 a 32 folhas).\n",
    "\n",
    "Similar ao AdaBoost o GradientBoost cria árvores de tamanho fixo baseado nos erros da árvore anterior, entretanto cada árvore pode ser maior que um stump. GB também escalam as árvores. E o GB continua a construir as árvores até o número máximo especificado, ou novas árvores falhem em melhorar o score.\n",
    "\n",
    "Todos os modelos de _Boosting_ partem da mesma estratégia utilizando árvores mais simples e estimando erros. A principal diferença dos diversos modelos de _Boosting_ que existe são justamente o algoritmo de otimização do modelo.\n",
    "\n",
    "O primeiro passo é calcular o pseudo-residual (similar a regressão linear, chamamos de pseudo-residual para lembrar que estamos utilizando o GB e não uma regressão linear). Com base no pseudo-residual, é calculado os gradientes (derivadas).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "No caso de modelos como o __GradientBoosting__, o principal diferencial é justamente utilizar um otimizador através de __gradiente descente__. Conforme o modelo faz uma iteração e cálcula a função de custo do modelo, utiliza-se de gradientes para definir o ajuste necessário na função inicial e repitir o processo iterativo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doJuF-UWx2Io"
   },
   "source": [
    "<img src=\"http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" width=500>\n",
    "\n",
    "<br>\n",
    "    \n",
    "   <img src=\"data:image/jpeg;base64,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\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rk0K2HyJUyn"
   },
   "source": [
    "Diferente do AdaBoost e do GradientBoost, o XGBoost é um algoritmo complexo com muitos passos envolvidos.  \n",
    "\n",
    "Em suma ele utiliza:\n",
    "- Grandient Boost\n",
    "- Regularização\n",
    "- Approximate Greedy Algorithm\n",
    "- Parallel Learning\n",
    "- Weighted Quantile Sketch\n",
    "- Sparsity-Aware split finding\n",
    "- Cache-Aware Access\n",
    "- Blocks for Out-of-Core Computation\n",
    "\n",
    "\n",
    "Aqui está uma série de videos para compreender melhor o XGBoost e como ele funciona.  \n",
    "\n",
    "[Video 1](https://www.youtube.com/watch?v=OtD8wVaFm6E)  \n",
    "[Video 2](https://www.youtube.com/watch?v=8b1JEDvenQU)  \n",
    "[Video 3](https://www.youtube.com/watch?v=ZVFeW798-2I)  \n",
    "[Video 4](https://www.youtube.com/watch?v=oRrKeUCEbq8)  \n",
    "\n",
    "Podemos encaixar quais algoritmos explorar em detalhes no fim do curso.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvtb_NRtx2Iq"
   },
   "source": [
    "No caso do __XGBoost__, o nome significa _Extreme Gradient Boosting_, ou seja ele utiliza de otimizações e uma arquitetura mais robusta para desenvolver resultados mais eficientes e com um custo computacional menor!\n",
    "\n",
    "<br>\n",
    "\n",
    "Este modelo junto com o _LightGBM_, são os modelos que dominaram as competições do _Kaggle_, se mostrando com os modelos mais eficientes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4433,
     "status": "ok",
     "timestamp": 1644012048555,
     "user": {
      "displayName": "Gilberto Kaihami",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg7889GQh1x2Zx39afauWPJZMRy7nevpgXX_pwq7Q=s64",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "pkKlXHMnx2Iq",
    "outputId": "d983b929-2c62-4c79-9669-a6f0a06f1304"
   },
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_rEmFyyx2Ir"
   },
   "source": [
    "Já o caso do __LightGBM__, foi um _framework_ otimizado do _Gradient Boosting_ desenvolvida pela _Microsoft Research_ e que impactou significativamente as competições de _Machine Learning_ com alta performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3039,
     "status": "ok",
     "timestamp": 1644012052077,
     "user": {
      "displayName": "Gilberto Kaihami",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg7889GQh1x2Zx39afauWPJZMRy7nevpgXX_pwq7Q=s64",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "FYIGmeeqx2Ir",
    "outputId": "42a1e1aa-1f2d-47ef-a746-3fffa71d7963"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3_mqoZx2Is"
   },
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5KJuBdgx2It"
   },
   "source": [
    "__1)__ Realize uma classificação com da coluna y dos dados de marketing bancário (contido no arquivo `bank-full.csv`) utilizando mais de um modelo dentro dos modelos conhecidos por vocês. Qual o melhor modelo a ser implementado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsUsX9uVx2It"
   },
   "source": [
    "__2)__ Com os dados do arquivo `Admission_Predict.csv` realize uma classificação dos alunos que possuem chance maior que 0.8 de admissão e dos que possuem chance menor que 0.8. Compare diversos modelos e determine qual seria o melhor a ser implementado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI8QYsAWx2It"
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WkwNv3DGx2Im",
    "yGozV9hlx2In",
    "rC3_mqoZx2Is"
   ],
   "name": "Aula 3 Ensembles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
