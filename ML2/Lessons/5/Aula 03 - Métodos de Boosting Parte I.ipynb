{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 03 - Métodos de Boosting (Parte I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos a seguinte situação: precisamos modelar as vendas de um determinado produto para fazer previsões, o mais precisas quanto possível, das vendas por mês. Como poderíamos abordar este problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um primeiro momento, poderíamos, por exemplo, pensar em utilizar a média de vendas em um determinado período (por exemplo, nos últimos 12 meses) como o valor a ser predito todos os meses. No entanto, é evidente que a performance desta abordagem deixaria muito a desejar, já que estaríamos simplesmente \"chutando\" a média de vendas o tempo todo. Contudo, pode nos servir como ponto de partida, e levantar o seguinte questionamento: *e se utilizássemos os erros do modelo como novas informações para o treinamento de um outro modelo?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é o princípio do **boosting:** utilizamos os erros no aprendizado de um modelo para treinar modelos subsequentes. Deste modo, a ideia é que \"os modelos passem a aprender com os erros passados\" e possa, assim, conjuntamente, performar melhor. Ou seja: *mesmo que a performance de cada um dos estimadores, individualmente, não seja boa, em conjunto, com o boosting, eles deveriam prover uma performance mais satisfatória!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Trabalho em conjunto\": Bagging x Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos algumas maneiras de criar *comitês* de estimadores. A um grupo de modelos atuando conjuntamente, seja para fins de classificação ou regressão, chamamos comumente de **ensemble** (conjunto). A ideia de utilizar diversos estimadores conjuntamente tem por **objetivo combinar modelos mais simples em um único modelo mais robusto, a fim de reduzir o viés, a variância e/ou aumentar a acurácia**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='padding:0.1em; background-color:#E9D8FD; color:#69337A'>\n",
    "<span>\n",
    "<h2>Algumas conceitualizações...</h2>\n",
    "<ul>\n",
    "<li>Podemos dizer que um modelo é um <b>weak learner</b> quando sua performance não é muito além de um \"chute aleatório\". <br><br>\n",
    "    <li>Por outro lado, um <b>strong learner</b> possui alta performance e boa capacidade de generalização;<br><br>\n",
    "<li>A ideia de métodos de ensemble é <b>combinar vários classificadores weak learners<\\b> para gerar boas performances a custos computacionais mais baixos.\n",
    "</ul>\n",
    "</span>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguns tipos de Ensemble:\n",
    "- __1. Bagging (short for bootstrap aggregation)__: Treina paralelamente $N$ modelos mais fracos (geralmente do mesmo tipo) com $N$ subsets distintos criados com amostragem randômica e reposição. Cada modelo é avaliado na fase de teste com o label definido pela moda (classificação) ou pela média dos valores (regressão). Os métodos de Bagging reduzem a variância da predição. <br>\n",
    "Algoritimos  famosos: Random Forest <br>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:828/format:webp/1*_pfQ7Xf-BAwfQXtaBbNTEg.png' style=\"width:600px\"  text=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*_pfQ7Xf-BAwfQXtaBbNTEg.png\" />  \n",
    "<br>\n",
    "<br>\n",
    "- __2. Boosting__: Treina $N$ modelos mais fracos (geralmente do mesmo tipo) de **forma sequencial**. Os pontos que foram classificados erroneamente recebem um peso maior para entrar no próximo modelo. Na fase de teste, cada modelo é avaliado com base do erro de teste de cada modelo, e a predição é feita com um peso sobre a votação. Os métodos de Boosting reduzem o viés da predição. <br>\n",
    "Algoritimos  famosos: AdaBoost, Gradient Boosting, XGBoost, CatBoost, LightGBM (Light Gradient Boosting Machine) <br>\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png' style=\"width:600px\" text=\"Fonte: https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png\" />\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### Algumas leituras complementares:\n",
    "[What is the difference between Bagging and Boosting?](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/)\n",
    "\n",
    "[Bagging vs Boosting in Machine Learning](https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting : AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O AdaBoost significa **Adaptive Boosting**, e tem como procedimento geral **a criação sucessiva dos chamados weak learners**, que são modelos bem fracos de aprendizagem - geralmente, **árvores de um único nó (stumps)**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1744/1*nJ5VrsiS1yaOR77d4h8gyw.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O AdaBoost utiliza os **erros da árvore anterior para melhorar a próxima árvore**. As predições finais são feitas com base **nos pesos de cada stump**, cuja determinação faz parte do algoritmo!\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" width=700>\n",
    "\n",
    "Vamos entender um pouco melhor...\n",
    "\n",
    "Aqui, o bootstrapping não é utilizado: o método começa treinando um classificador fraco **no dataset original**, e depois treina diversas cópias adicionais do classificador **no mesmo dataset**, mas dando **um peso maior às observações que foram classificadas erroneamente** (ou, no caso de regressões, a observações **com o maior erro**).\n",
    "\n",
    "Assim, após diversas iterações, classificadores/regressores vão sequencialmente \"focando nos casos mais difíceis\", e construindo um classificador encadeado que seja forte, apesar de utilizar diversos classificadores fracos em como elementos fundamentais.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zhuo_Wang8/publication/288699540/figure/fig9/AS:668373486686246@1536364065786/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png\" width=500>\n",
    "\n",
    "De forma resumida, as principais ideias por trás deste algoritmo são:\n",
    "\n",
    "- O algoritmo cria e combina um conjunto de **modelos fracos** (em geral, stumps);\n",
    "- Cada stump é criado **levando em consideração os erros do stump anterior**;\n",
    "- Alguns dos stumps têm **maior peso de decisão** do que outros na predição final;\n",
    "\n",
    "As classes no sklearn são:\n",
    "\n",
    "- [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "- [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)\n",
    "\n",
    "Note que não há muitos hiperparâmetros. O mais importante, que deve ser tunado com o grid/random search, é:\n",
    "\n",
    "- `n_estimators` : o número de weak learners encadeados;\n",
    "\n",
    "Além disso, pode também ser interessante tunar os hiperparâmetros dos weak learners. Isso é possível de ser feito, como veremos a seguir!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como exemplo, vamos utilizar uma base de dados sobre [risco de crédito](https://www.kaggle.com/datasets/uciml/german-credit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('german_credit_data.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:55:12.207316Z",
     "start_time": "2022-06-03T00:55:12.190322Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:53:38.415845Z",
     "start_time": "2022-06-03T00:53:38.401888Z"
    }
   },
   "outputs": [],
   "source": [
    "def pipe_pre_process():\n",
    "    # Para facilitação de carregamento e transformação de dados\n",
    "    \n",
    "    # Carregamento de dados\n",
    "    df = pd.read_csv('german_credit_data.csv')\n",
    "    \n",
    "    # Definição de features e target\n",
    "    X = df.drop(columns='Risk')\n",
    "    y = df[\"Risk\"]\n",
    "    \n",
    "    # Particionamento dos conjuntos de treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "    \n",
    "    #================================\n",
    "    # Tratamento dos dados numéricos\n",
    "    pipe_features_num = Pipeline([(\"input_num\", SimpleImputer(strategy = \"mean\")), # substituindo valores nulos pela média\n",
    "                                 (\"scaler\", StandardScaler())]) # padronização dos dados\n",
    "    \n",
    "    \n",
    "    features_num = X_train.select_dtypes(include=np.number).columns.tolist() # selecionando apenas as colunas numéricas\n",
    "    #==============================\n",
    "    # Tratamento de features categóricas\n",
    "    pipe_features_cat = Pipeline([(\"input_cat\", SimpleImputer(strategy = \"constant\", fill_value = \"unknown\")), # dados categóricos faltantes\n",
    "                                 (\"onehot\", OneHotEncoder())]) # transformação de dados categóricos\n",
    "    \n",
    "    features_cat = X_train.select_dtypes(exclude=np.number).columns.tolist() # selecionando apenas colunas categóricas\n",
    "    #==============================\n",
    "    # Aplicando as transformações nos nossos dados\n",
    "    pre_processador = ColumnTransformer([(\"transf_num\", pipe_features_num, features_num),\n",
    "                                        (\"transf_cat\", pipe_features_cat, features_cat)])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pre_processador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:45:00.841206Z",
     "start_time": "2022-06-03T00:45:00.819079Z"
    }
   },
   "outputs": [],
   "source": [
    "def metricas_classificacao(estimator):\n",
    "    #=================\n",
    "    print(\"\\nMétricas da avaliação de treino:\")\n",
    "    \n",
    "    y_pred_train = estimator.predict(X_train) # predição sobre os dados de treinamento\n",
    "    \n",
    "    print(confusion_matrix(y_train, y_pred_train)) # matriz de confusão\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(y_train, y_pred_train)\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    \n",
    "    #=================\n",
    "    print(\"\\nMétricas da avaliação de teste:\")\n",
    "    \n",
    "    y_pred_test = estimator.predict(X_test) # predição sobre os dados de treinamento\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_pred_test)) # matriz de confusão\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test)\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AdaBoost classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:52:22.558153Z",
     "start_time": "2022-06-03T00:52:21.539859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, pre_processador = pipe_pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:53:54.312020Z",
     "start_time": "2022-06-03T00:53:54.071391Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline para o Adaboost\n",
    "pipe_ab = Pipeline([(\"pre_processador\", pre_processador),\n",
    "                   (\"ab\", AdaBoostClassifier(random_state = 42))])\n",
    "\n",
    "# por padrão, n_estimators = 50\n",
    "pipe_ab.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:55:18.066517Z",
     "start_time": "2022-06-03T00:55:17.477866Z"
    }
   },
   "outputs": [],
   "source": [
    "metricas_classificacao(pipe_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando $n_{estimators} = 150$ (apenas um \"chute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:58:06.677561Z",
     "start_time": "2022-06-03T00:58:06.103584Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline para o Adaboost\n",
    "pipe_ab = Pipeline([(\"pre_processador\", pre_processador),\n",
    "                   (\"ab\", AdaBoostClassifier(random_state = 42, n_estimators = 150))])\n",
    "\n",
    "# por padrão, n_estimators = 50\n",
    "pipe_ab.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T00:58:14.683093Z",
     "start_time": "2022-06-03T00:58:14.086442Z"
    }
   },
   "outputs": [],
   "source": [
    "metricas_classificacao(pipe_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em grupos, trabalhem sobre o dataset 'german_credit_data' para tentar melhorar a performance do modelo com o adaboost. Que maneiras vocês enxergam de fazê-lo? Como implementá-las?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
